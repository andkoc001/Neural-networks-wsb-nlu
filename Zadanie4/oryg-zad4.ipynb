{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zadanie 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozpatrzymy przykład sieci, której zadaniem będzie klasyfikacja pary liczb do dwóch grup. Albo grupy, której wynik operacji XOR da 0 lub grupy, której wynik operacji XOR da 1. XOR polega ona na przyjęciu: dwóch liczb (zazwyczaj są to liczby naturalne 0 lub 1) oraz tego, że oczekiwany wynik jest równy zeru, gdy dwie liczby są takie same. Dla par (0,0) i (1,1) otrzymujemy na wyjściu 0, natomiast dla par różniących się (0,1) lub (1,0) otrzymujemy na wyjściu 1. Problem pojawia się w momencie, gdy założymy, że nasze liczby nie są liczbami całkowitymi, to znaczy jak mamy interpretować parę liczb (0.1,1.3)? Wyobraźmy sobie, że musimy dokonać operacji XOR dwóch wejść, które podają napięcie z zakresu od 0V do 1.3V. To napięcie prawie zawsze nie będzie idealnie równe liczbie całkowitej. Dla takiego układu musimy stworzyć model, który będzie potrafił sklasyfikować (stąd nazwa sieć klasyfikacyjna lub klasyfikator) daną parę liczb do dwóch grup – albo do grupy par liczb, dających w wyniku operacji XOR 0 lub do grupy par liczb, które w wyniku operacji XOR dadzą 1. Będziemy realizować cel wygenerowania losowych danych, na których dokonamy późniejszego treningu (uczenia) naszej sieci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd53575a-47ed-4285-bc95-d60a87cecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "class XORDataCreator(data.Dataset):\n",
    "   \"\"\"  XORDataCreator jest klasą, która generuje losowe dane treningowe dla głębokiej sieci klasyfikacyjnej.\n",
    "    Klasyfikator, który korzysta z generowanych danych powinien otrzymać parę liczb zmiennoprzecinkowych obarczonych szumem oraz oznaczenie pary liczb jakiego wyniku alternatywy rozłącznej (operacja XOR) spodziewamy się po tej parze. Na tej podstawie sieć ma nauczyć się rozpoznawać wyniki z możliwie jak najwyższą poprawnością nieznanej dotąd pary liczb zmiennoprzecinkowych.\"\"\"\n",
    "    # Definicja konstruktora (metoda inicjalizacyjna klasy)\n",
    "    # data_size: ilość danych losowych, która ma zostać wygenerowana przez obiekt tej klasy\n",
    "    # noise_std_deviation: odchylenie standardowe szumu Gaussowskiego (zaszumienie danych ma na celu przygotować trening sieci na mniej standardowe dane)\n",
    "    def __init__(self, data_size, noise_std_deviation) -> None:\n",
    "        super().__init__()\n",
    "        self.data_size = data_size\n",
    "        self.noise_std_deviation = noise_std_deviation\n",
    "        self.generate_random_xor_data()\n",
    "    def generate_random_xor_data(self):\n",
    "        # Generacja losowych par liczb (x,y) z przedziału <0,2>\n",
    "        xor_data = torch.randint(low=0, high=2, size=(self.data_size, 2), dtype=torch.float32)\n",
    "        # Generacja szumu dla danych (losowy tensor o tych samych wymiarach co nasze dane wygenerowane linię wyżej z zakresu <0,1> pomnożony przez odchylenie standardowe)\n",
    "        xor_data_noise = ???\n",
    "        # Labelling danych oznacza nadanie każdemu elementowi z tablicy danych oczekiwanego wyniku\n",
    "        # W tym przypadku aby określić oczekiwany wynik dokonujemy operacji XOR (operator ^).\n",
    "        labels = ???\n",
    "        # Zapisujemy wygenerowane dane do pól klasy prezentujących zbiór danych treningowych oraz nadanym im oczekiwanych wyników (labels)\n",
    "        xor_data_with_noise = ???\n",
    "        labels_converted_to_numbers = labels # Lista 'labels' zawiera jednoelementowe tensory - poprzez metodę item() wyłuskujemy wartość\n",
    "        self.data_inputs = xor_data_with_noise\n",
    "        self.data_labels = [label.to(torch.long) for label in labels]\n",
    "    def __len__(self):\n",
    "        return ???\n",
    "    def __getitem__(self, index):\n",
    "        return ???\n",
    "# Użycie wyżej zdefiniowanej klasy do generacji 300 elementowego tensora danych\n",
    "xor_dataset = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po uruchomieniu naszego program zmienna xor_dataset powinna zawierać tensor (a dokładniej parę określaną jako tuple) dwóch elementów – jeden z nich to kolejny tensor, który zawiera zbiór 300 elementów par losowo wygenerowanych liczb, a drugi element to oczekiwany wynik operacji XOR pary liczb z pierwszego tensora. Oczekiwany wynik jest podawany w sieci, aby ta mogła na bieżąco „dostrajać” wagi w tym przypadku dwóch wejść (na wejściu mamy parę liczb) tak, aby wyjście z jak największym prawdopodobieństwem pokrywało się z tym, czego oczekujemy na podstawie wygenerowanych danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ak widać, pary liczb, które są różne (zaokrąglają się do różnych liczb całkowitych) i wedle operacji XOR dają wynik 1 (pomarańczowe punkty), znajdują się w lewej górnej oraz prawej dolnej części wykresu, przeciwnie do par, które zaokrąglają się do tych samych wartości. Teraz zajmiemy się próbą zdefiniowania architektury oraz działania naszego modelu głębokiej sieci neuronowej tak, aby ta mogła dla dowolnego punktu danego tej sieci (nawet takiego, którego nie było w danych, na których sieć była uczona) określić, do jakiej grupy należy.\n",
    "\n",
    "Oczywiście, ten problem jest stosunkowo prosty celem zaprezentowania zwięzłego przykładu, ale każdy problem, który da się opisać liczbowo, może później zostać poddany próbie nauczenia sieci tego, w jaki sposób te dane są powiązane z informacją końcową, która interesuje użytkownika (na przykład wyniki badań krwi z daną jednostką chorobową)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skupimy się teraz na zbudowaniu modelu sieci neuronowej oraz zdefiniowaniu procesu treningu (uczenia) sieci. Pierwszym szczegółem jest użycie modułu torch.nn – skrót „nn” pochodzi od pojęcia neural network. Jest to moduł, który pozwala nam w łatwy sposób zdefiniować strukturę naszego modelu sieci neuronowej. W naszym przypadku, ponieważ jest to dość prosty problem, jest to struktura składająca się z:\n",
    "    • Warstwy wejściowej (wejściowa para liczb),\n",
    "    • Warstwy zwanej ukrytą (wszystkie warstwy głębokich sieci neuronowych poza warstwą wejściową i wyjściową nazywane są zwyczajowo ukrytymi),\n",
    "    • Warstwą wyjściową, która prezentuje już interesujący nas wynik.\n",
    "Poniżej zdefiniowana klasa ClassifierModel definiuje model naszej głębokiej sieci klasyfikatora. \n",
    "W konstruktorze za argumenty wejściowe przyjmujemy ilość neuronów poszczególnych warstw (ilość komórek przyjmujących dane oraz propagujących je dalej w przetworzonej formie). Skupmy się na wyjaśnieniu zdefiniowanych relacji między warstwami – tutaj widać konkretne odwołanie do modułu nn zawierający wiele modeli klas, które pozwalają nam w prosty i przejrzysty sposób zaprojektować naszą sieć."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    def __init__(self, inputs_number, hidden_layer_neurons_number, outputs_number) -> None:\n",
    "        super().__init__()\n",
    "        self.first_layer_to_hidden_transformation = nn.Linear(inputs_number, hidden_layer_neurons_number) \n",
    "        self.activation_function = ???\n",
    "        self.hidden_layer_to_output_transformation = ???\n",
    "    def forward(self, input):\n",
    "        x = self.first_layer_to_hidden_transformation(input)\n",
    "        x = self.activation_function(x)\n",
    "        x = self.hidden_layer_to_output_transformation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idąc kolejno po każdej linii naszej klasy, napotykamy na definicję pierwszej transformacji (na czerwono). Definiujemy tę transformację jako liniową (nn.Linear), czyli taką, która jest standardową definicją wiążącą wejścia, które są ważone (nadawane są im wagi, czyli liczby, przez które są mnożone wejścia, a ustalane są przez sieć w procesie uczenia tak, aby jak najlepiej odzwierciedlać wyuczonym modelem opisaną w danych treningowych przez nas sytuację), a zaprezentowane mogą zostać za pomocą dość prostego wzoru funkcji liniowej. Funkcja liniowa poprzez odpowiedni dobór wag w wektorze W stara się zminimalizować błąd (pomyłkę) modelu sieci na podstawie dostarczonych danych treningowych. Opisywany problem jest dość prosty i da się opisać analitycznie w sposób liniowy, natomiast niestety większość musi zostać opisana za pomocą nieliniowych funkcji. Następną ważną składową naszej sieci jest funkcja aktywacji, która w naszym przypadku jest funkcją równą tangensowi hiperbolicznemu (nn.Tanh()). Wartości naszej funkcji aktywacji wchodzą w zakres pomiędzy -1 a 1. Jest to świetna funkcja, która wprowadza poprzez swoją nieliniowość możliwość rozwiązania nieliniowych problemów. W dodatku jest wycentrowaną funkcją, ponieważ dla argumentu zero również przyjmuje wartość równą zeru — przecina osie dokładnie w punkcie (0,0). Z tego powodu, podczas gdy nasza sieć się uczy i próbuje poznać, jak wrażliwa jest funkcja błędu (pomyłki) na wspomniane wcześniej wagi każdego z wejść (licząc gradient tej funkcji), korzysta z przywileju, iż gradient tangensu hiperbolicznego jest symetryczny względem osi zerowej (y = 0).\n",
    "\n",
    "To wielka przewaga tej funkcji nad wieloma pozostałymi nieliniowymi funkcjami aktywacji. Jednakże różne klasy problemów do rozwiązania mają swoje empiryczne rekomendacje odnośnie zastosowania konkretnej funkcji aktywacji. W internecie możemy spotkać mnóstwo dobrej jakości poradników w tym zakresie i warto z nich korzystać, ponieważ zazwyczaj są one poparte praktyką.\n",
    "\n",
    "Ostatnią transformacją wiążącą warstwę ukrytą i warstwę wyjściową jest również relacja liniowa. Funkcja forward zdefiniowana w tej samej klasie ma za zadanie przyjąć wektor wejściowy i „przepuścić” go przez kolejne warstwy, po czym zwrócić tensor wyjściowy. Taki tensor jest dla nas po prostu informacją o tym, jaki wynik operacji XOR otrzymamy. Wynikiem będzie liczba zmiennoprzecinkowa pomiędzy 0 a 1, gdyż wynik operacji XOR to właśnie w idealnym przypadku 0 lub 1 – tutaj sieć o tym nie wie, dlatego z pewnym prawdopodobieństwem przybliża się ku zeru lub jedynce, stąd zmiennoprzecinkowość.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W skrócie, cały proces przygotowania oraz treningu naszej sieci będzie przebiegał w następujących krokach:\n",
    "\n",
    "Inicjalizacja klasy odpowiedzialnej za dostarczenie danych treningowych.\n",
    "Pobranie częściowej paczki danych z całościowej kolekcji danych treningowych sieci.\n",
    "Obliczenie przez obecny model sieci wyjścia dla zaprezentowanych danych wejściowych.\n",
    "Obliczenie błędu, czyli różnicy pomiędzy wynikiem oczekiwanym (podanym na przykład przez nas w klasie odpowiedzialnej za dostarczenie danych – będzie to na przykład dla pary liczb (0,1) oczekiwany wynik 1, ponieważ operacja XOR dla tej pary daje właśnie taki wynik) a tym, jaki wynik otrzymaliśmy w punkcie 3.\n",
    "Dokonanie propagacji wstecznej błędu, czyli sprawdzenie na podstawie funkcji gradientu (czyli funkcji prezentującej wrażliwość funkcji błędu na wagi nadane konkretnym wejściom), w którym punkcie jesteśmy (czy zwiększając na przykład wagę danego wejścia zwiększymy, czy zmniejszymy błąd – będziemy zawsze chcieli go zmniejszyć).\n",
    "Aktualizacja wag tak, aby zmniejszyć błąd sieci.\n",
    "Gdy błąd jest na tyle mały, że uznamy go za satysfakcjonujący, zapisujemy model sieci.\n",
    "Punkty te będziemy powtarzać cyklicznie aż osiągniemy zadowalającą dokładność przewidywania wyniku przez model sieci.\n",
    "Przejdźmy zatem do kluczowej części kodu, czyli treningu naszej sieci. W tym celu, posługując się powyższą instrukcją, przeanalizujemy kod, którego zadaniem będzie w ogólności trening naszego modelu sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from classifier_model import ClassifierModel\n",
    "from data_loader import XORDataCreator\n",
    "\n",
    "class Net():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.loss_calculation_model = None\n",
    "        self.optimizer = None\n",
    "        self.data_loader = XORDataCreator(data_size=100, noise_std_deviation=0.1)\n",
    "\n",
    "    def train(self, model: ClassifierModel, epochs_num: int=50):\n",
    "        # Sprawdźmy czy możemy wykorzystać GPU poprzez pakiet CUDA celem przyspieszenia obliczeń\n",
    "        gpu_available = torch.cuda.is_available()\n",
    "        # W przypadku możliwości ustawmy 'device' na GPU\n",
    "        device = torch.device('cuda') if gpu_available else torch.device('cpu')\n",
    "\n",
    "        # Inicjalizacja\n",
    "        self.loss_calculation_model = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "        # Załadujmy nasz model do GPU jeśli jest dostępne\n",
    "        model.to(device)\n",
    "        # Ustawmy model w tryb treningowy (funkcjonalność odziedziczona po klasie nn.Module)\n",
    "        model.train()\n",
    "\n",
    "        # Pętla treningowa (pobierz paczkę danych treningowych)\n",
    "        for current_epoch in range(epochs_num):\n",
    "            epoch_loss = 0.0\n",
    "            for data_input, data_label in self.data_loader:\n",
    "                # Załadujmy nasze dane do GPU jeśli jest w użyciu\n",
    "                data_input = data_input.to(device)\n",
    "                data_label = data_label.to(device)\n",
    "\n",
    "                # Przeprocedujmy dane wejściowe przez nasz model\n",
    "                predicted_output = ???\n",
    "                predicted_output = ???\n",
    "\n",
    "                # Obliczmy wartość funkcji straty (jak bardzo nasz model się pomylił)\n",
    "                loss = self.loss_calculation_model(predicted_output, data_label.float())\n",
    "\n",
    "                # Wyzerujmy wartość gradientu na wszelki wypadek\n",
    "                ???\n",
    "\n",
    "                # Dokonajmy propagacji wstecznej błędu\n",
    "                ???\n",
    "\n",
    "                # Zaktualizujmy wagi sieci na podstawie obliczonego gradientu\n",
    "                ???\n",
    "\n",
    "                # Obliczenie średniej pomyłki obecnej iteracji treningowej\n",
    "                epoch_loss += ???\n",
    "\n",
    "            # Logowanie istotnych danych odnośnie zakończonej iteracji\n",
    "            print('epoch: ', ???)\n",
    "            print('average epoch loss: ', ???)\n",
    "\n",
    "    def set_optimizer(self, optimizer) -> None:\n",
    "        self.optimizer = ???\n",
    "\n",
    "    def set_loss_calc_model(self, loss_calc_model) -> None:\n",
    "        self.loss_calculation_model = ???\n",
    "\n",
    "    def get_training_data(self) -> XORDataCreator:\n",
    "        return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zazwyczaj trening skomplikowanych modeli sieci neuronowych jest czasochłonny – zatem pozostaje pytanie, co można zrobić, aby nie utracić danych treningowych w losowych przypadkach lub aby móc podzielić ten czasochłonny proces na kilka mniejszych? Oczywiście można zapisać parametry tymczasowego modelu do pliku, a następnie wczytać ostatnie zapisane podczas ponownej sesji treningowej. Aby wczytać oraz zapisać stan bieżący modelu, należy użyć wbudowanych metod pakietu torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "import torch\n",
    "from net import Net\n",
    "from classifier_model import ClassifierModel\n",
    " \n",
    "__clasifier_model_state_file_name__ = \"classifier_model.pt\"\n",
    " \n",
    "classifier_model = ClassifierModel(???)\n",
    " \n",
    "# Wczytaj poprzednio zapisany model, jeśli istnieje\n",
    "if exists(__clasifier_model_state_file_name__):\n",
    "    model_state = torch.load(???)\n",
    "    classifier_model.load_state_dict((???)\n",
    "\n",
    "\n",
    "net_model = ???\n",
    "net_model.train_and_log(model=???)\n",
    " \n",
    "# Zapisz dotychczasowy model\n",
    "model_state = classifier_model.state_dict()\n",
    "torch.save(???)\n",
    "print(\"Trained model state dictionary saved to \", ???)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
