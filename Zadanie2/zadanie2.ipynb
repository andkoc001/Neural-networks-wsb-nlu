{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fd7979-e95b-4525-9ffa-de59b5331116",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "Lecturer: dr Andrzej Tomski, WSB-NLU  \n",
    "\n",
    "> Author: Andrzej Kocielski, 2023-2024  \n",
    "> email: <akocielski@student.wsb-nlu.edu.pl>, <and.koc001@gmail.com>\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3385db-4aba-4bd1-bb7b-c169d4de92f1",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Built an ANN to estimate price, using provided dataset, \"CarPricesData.pkl\".\n",
    "\n",
    "Assumptions:\n",
    "- The provided data is to be split into training and test sets.\n",
    "- Sequential model from Keras is to be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755e9181-cb09-475c-985e-d626e417b49f",
   "metadata": {},
   "source": [
    "### Importing libraries and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771be49-d7d1-453a-9444-6de81118aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import warnings # ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # used for standaristion of data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "from keras.models import Sequential # model of ANN\n",
    "from keras.layers import Dense # layers of ANN\n",
    "\n",
    "# model metrics\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6c942-4b50-4a94-821e-736fe869038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove the scientific notation from numpy arrays\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2400b801-2266-4977-a78c-6f4692a60824",
   "metadata": {},
   "source": [
    "### Load and inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e202f7-283e-44bd-9831-d1cd981d20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from file\n",
    "CarPricesData = pd.read_pickle('CarPricesData.pkl')\n",
    "# Data inspection\n",
    "print(f\"Shape of dataframe: {CarPricesData.shape}\\n\")\n",
    "print(CarPricesData.info(), \"\\n\")\n",
    "print(CarPricesData.head(2))\n",
    "print(CarPricesData.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc1da70-2463-45da-bdc2-e28f0fbb8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# sns.boxplot(CarPricesData[\"KM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f7d66-cd0d-41c8-b967-434f3957cfd6",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8822c05b-79bf-4b14-9705-1f31b6f8d084",
   "metadata": {},
   "source": [
    "It appears there is some inaccurate data, e.g. with possible outliers in KM column. \n",
    "\n",
    "All the records with KM = 1 will be removed (even though in some cases the Age = 1, which could be legitimate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076dccc-30e4-4ace-a50a-3a06edc43e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CarPricesData[CarPricesData[\"KM\"] < 1000]\n",
    "cleaned_data = CarPricesData[CarPricesData[\"KM\"] != 1]\n",
    "print(f\"Removed rows: {CarPricesData.shape[0] - cleaned_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a0214-889d-43aa-9cf7-8a3e851927aa",
   "metadata": {},
   "source": [
    "As the goal of this NN is to predict Price based on the other parameters, this information must be removed from the dataset destined for training the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a67c60-aa58-4745-a3aa-3e529c02419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate Target Variable and Predictor Variables\n",
    "TargetVariable = ['Price']\n",
    "Predictors = ['Age', 'KM', 'Weight', 'HP', 'MetColor', 'CC', 'Doors']\n",
    "\n",
    "X = cleaned_data[Predictors].values\n",
    "y = cleaned_data[TargetVariable].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ec926e-b9d0-4718-a4cb-9a9f964367fc",
   "metadata": {},
   "source": [
    "Sandardization of data, using `StandardScaler` from sklearn.preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5ffb7-da96-4f3f-8ebd-15a72143ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set StandardScaler to both datasets\n",
    "PredictorScaler = StandardScaler()\n",
    "TargetVarScaler = StandardScaler()\n",
    "\n",
    "# Storing the fit object for later reference\n",
    "PredictorScalerFit = PredictorScaler.fit(X)\n",
    "TargetVarScalerFit = TargetVarScaler.fit(y)\n",
    " \n",
    "# Generating the standardized values of X and y\n",
    "X = PredictorScalerFit.transform(X)\n",
    "y = TargetVarScalerFit.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c79c7-7dbd-4a3e-bf12-14b4cdec2441",
   "metadata": {},
   "source": [
    "Split the data into training and testing sets, using method `train_test_split` from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282edb82-c7fb-4516-a94f-1712a2c1be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 70% training and 30% testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2023)\n",
    " \n",
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6c7c8-256f-49e5-98f8-af5358150079",
   "metadata": {},
   "source": [
    "### Build ANN\n",
    "\n",
    "Hiperparameters as per exercise notes:  \n",
    "- units=5: This means that we are creating a layer with five neurons. Each of these five neurons will receive input values, for example the \"Age\" values will be passed to all five neurons, as will all other columns.\n",
    "- input_dim=7: This means that there are seven predictors in the input that are expected by the first layer. If you see a second dense layer, we do not specify this value because the sequential model passes this information on to subsequent layers.\n",
    "- kernel_initializer='normal': When the neurons start computing, some algorithm must decide the value for each weight. This parameter specifies that we can choose different values for it, such as \"normal\" or \"glorot_uniform\".\n",
    "- activation='relu': Specifies the activation function for the computations inside each neuron. You can choose values such as \"relu\", \"tanh\", \"sigmoid\", etc.\n",
    "\n",
    "\n",
    "Further parameters for tuning the model:\n",
    "- batch_size=10: Specifies how many rows will be fed to the network at one time, after which SSE calculation will begin and the neural network will begin to adjust its weights based on errors.\n",
    "- epochs=20: The same weight adjustment action is continued 50 times according to this parameter. To put it simply, the ANN looks at the full training data 50 times and adjusts its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7bfe0-fd3d-4b69-9707-e3deefff35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ANN model\n",
    "model = Sequential()\n",
    "\n",
    "# Building the layers of the net\n",
    "# Defining the Input layer and FIRST hidden layer, both are same!\n",
    "model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# Defining the Second layer of the model\n",
    "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "# model info\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e94df-accb-49cd-ad4a-702724c2a131",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0db2e1-c66c-428a-87b6-0b4aee2a5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=20,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3542b5f5-51f6-4e62-8e07-c92766a7ee1f",
   "metadata": {},
   "source": [
    "### Visualising the training progress\n",
    "\n",
    "In practice, when training a model using frameworks like Keras or similar machine learning libraries, there are typically two datasets: the training set (_train_) and the validation set (_val_ or _validation_). Metrics values, such as accuracy (_accuracy_) or loss function (_loss_), are monitored on both the training and validation sets.  \n",
    "\n",
    "In the code, `val_` is a prefix used to refer to the metric values on the validation (_test_) set during the training of the model. In this context, if `key1` is, for example, 'accuracy', then `'val_' + key1` denotes the accuracy on the validation set.  \n",
    "\n",
    "Therefore, when using `history.history['val_' + key1]`, refers to the accuracy values on the validation set, and `history.history[key1]` refers to the accuracy values on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1abbfb-c73e-4fb4-a419-260401dacee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_curves(history, key1='accuracy', ylim1=(0.0, 1.0), key2='loss', ylim2=(0.0, 0.5)):\n",
    "    \n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    # plt.subplot(1, 2, 1)\n",
    "    # plt.plot(history.history[key1], \"r--\")\n",
    "    # plt.plot(history.history['val_' + key1], \"g--\")\n",
    "    # plt.ylabel(key1)\n",
    "    # plt.xlabel('Epoch')\n",
    "    # plt.ylim(ylim1)\n",
    "    # plt.legend(['train', 'test'], loc='best')\n",
    "    \n",
    "    # plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[key2], \"r--\")\n",
    "    plt.plot(history.history['val_' + key2], \"g--\")\n",
    "    plt.ylabel(key2)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(ylim2)    \n",
    "    plt.legend(['train', 'test'], loc='best')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa867344-09de-405d-a91f-88e49979ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_curves(history, key1='accuracy', ylim1=(-0.1, 1.2), key2='loss', ylim2=(0.0, 1.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e291e9f6-6d6a-4d24-b014-6875b8501b28",
   "metadata": {},
   "source": [
    "## Find the best parameters for the ANN\n",
    "### Training the model\n",
    "\n",
    "Further parameters for tuning the model:\n",
    "- batch_size=20: Specifies how many rows will be fed to the network at one time, after which SSE calculation will begin and the neural network will begin to adjust its weights based on errors.\n",
    "- epochs=50: The same weight adjustment action is continued 50 times according to this parameter. To put it simply, the ANN looks at the full training data 50 times and adjusts its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19c679d-dad6-44fe-9ac3-4931b142611a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindBestParams(X_train, y_train, X_test, y_test):\n",
    "    \"\"\" \n",
    "    Function finds the best parameters for training the ANN.\n",
    "    \"\"\"    \n",
    "    # Defining the list of hyper parameters to try\n",
    "    batch_size_list = [5, 10]#, 15, 20]\n",
    "    epoch_list = [5, 10]#, 30, 60]\n",
    "    \n",
    "    SearchResultsData = pd.DataFrame(columns=['TrialNumber', 'Parameters', 'MAPE', 'Accuracy'])\n",
    "    \n",
    "    # initializing the trials\n",
    "    TrialNumber=0\n",
    "    for batch_size_trial in batch_size_list:\n",
    "        for epochs_trial in epoch_list:\n",
    "            print()\n",
    "            TrialNumber+=1\n",
    "            \n",
    "            # create ANN model\n",
    "            model = Sequential()\n",
    "            # Defining the first layer of the model\n",
    "            model.add(Dense(units=5, input_dim=7, kernel_initializer='normal', activation='relu'))\n",
    "            # Defining the Second layer of the model\n",
    "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
    "            # The output neuron \n",
    "            model.add(Dense(1))\n",
    "\n",
    "            # Compiling the model\n",
    "            model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "            \n",
    "            # Fitting the ANN to the Training set\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=epochs_trial,\n",
    "                batch_size=batch_size_trial,\n",
    "                validation_data=(X_test, y_test),\n",
    "                verbose=0,\n",
    "                )\n",
    "\n",
    "            MAPE = np.mean(100 * (np.abs(y_test - model.predict(X_test))/y_test))\n",
    "           \n",
    "            # printing the results of the current iteration\n",
    "            # print(f\"Trial {TrialNumber}, MAPE: {MAPE}\")\n",
    "\n",
    "            trial_result = pd.DataFrame(\n",
    "                data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), MAPE, 100-MAPE]],\n",
    "                              columns=['TrialNumber', 'Parameters', 'MAPE', 'Accuracy'])\n",
    "            print(trial_result)\n",
    "\n",
    "            SearchResultsData = pd.concat([SearchResultsData, trial_result], ignore_index=True)\n",
    "            # SearchResultsData = SearchResultsData.append(trial_result)) # depricated in Pandas 2.0\n",
    "    \n",
    "    return(SearchResultsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261047a-11e2-4293-a9db-947ab49fe598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "ResultsData = FindBestParams(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa778d1-fbb4-449d-bb7b-0cd5e8b9826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best accuracy\n",
    "max_accuracy = ResultsData.loc[ResultsData['Accuracy'].idxmax()]\n",
    "print(f\"Best accuracy found: {max_accuracy[-1]:.3f}, with corresonding batch size and number of epochs: {max_accuracy[1]}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23632a-649e-402e-928d-02361d9164b3",
   "metadata": {},
   "source": [
    "___\n",
    "## Linear regression model\n",
    "\n",
    "For comparison, linear regression model is used.\n",
    "\n",
    "For this model, the same dataset and subsets are used (X_train, X_test, y_train, y_test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ef46d0-e66f-4a89-b9f6-37ae3055bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity check with the shapes of Training and testing datasets\n",
    "print(f\"Train inputs {X_train.shape}, train outputs {y_train.shape}\")\n",
    "print(f\"Test inputs {X_test.shape}, test outputs {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ca850-cc15-4507-9183-74f951eeabcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of a LinearRegression() model named lin_reg_model.\n",
    "lin_reg_model = LinearRegression()\n",
    "\n",
    "#Train/fit lin_reg_model on the training data.\n",
    "lin_reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b4f65-213d-4941-a20b-c76f0e8bc08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = lin_reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6436cb-f112-44dc-a88d-2929b4db122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of the linear regression model\n",
    "MAPE_lr = np.mean(100 * (np.abs(y_test - predictions)/y_test))\n",
    "\n",
    "print(f\"Accuracy: {100 - MAPE_lr:.3f}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b560a9-4476-4fb0-ae50-73ae09a80919",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "Both models were compared using Mean Absolute Percentage Error (MAPE) metrics.  \n",
    "The formula for MAPE is:\n",
    "\n",
    "$$ MAPE = \\frac{1}{n} \\sum_{i=1}^{n} \\left| \\frac{A_i - F_i}{A_i} \\right| \\times 100\\% $$\n",
    "\n",
    "Where:  \n",
    "$ n $ is the number of observations; \n",
    "$ A_i $ is the actual value; \n",
    "$ F_i $ is the forecasted (predicted) value.\n",
    "\n",
    "Model accuracy is expressed as: $ 100\\% - MAPE $\n",
    "\n",
    "The following are the found accuracies of both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa0259-276d-429a-80b0-b4e35d212dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ANN accuracy:\\t {max_accuracy[-1]:.3f}%\")\n",
    "print(f\"LR accuracy:\\t {100 - MAPE_lr:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e663dde-bdb4-4c6f-a96c-5def92e016fc",
   "metadata": {},
   "source": [
    "References:\n",
    "- <https://www.analyticsvidhya.com/blog/2021/07/car-price-prediction-machine-learning-vs-deep-learning/>\n",
    "- <https://www.kaggle.com/code/karan842/car-price-prediction-neural-network>\n",
    "- <https://github.com/andkoc001/Machine-Learning-and-Statistics-Project>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a526775-2e03-4f3d-8431-4a7a81711b21",
   "metadata": {},
   "source": [
    "___\n",
    "Andrzej Kocielski, 2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
